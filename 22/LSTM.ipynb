{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def panel(ax_sx, hist, panel_title, trn_val, dev_val):\n",
    "    ax_sx.set_title(panel_title)\n",
    "    ax_sx.plot(hist.history[trn_val], color='#1f77b4')\n",
    "    ax_sx.set_ylabel('train', color='#1f77b4')\n",
    "    ax_sx.tick_params('y', colors='#1f77b4')\n",
    "    ax_dx = ax_sx.twinx()\n",
    "    ax_dx.plot(hist.history[dev_val], color='#ff7f0e')\n",
    "    ax_dx.set_ylabel('validation', color='#ff7f0e')\n",
    "    ax_dx.tick_params('y', colors='#ff7f0e')    \n",
    "\n",
    "def show_training(history):\n",
    "    fig, (ax11, ax12) = plt.subplots(1, 2, figsize=(10, 4), dpi=300)\n",
    "    fig.suptitle('Training model', y=1.05)\n",
    "    panel(ax11, history, 'accuracy', 'acc', 'val_acc')\n",
    "    panel(ax12, history, 'loss', 'loss', 'val_loss')\n",
    "    fig.tight_layout()\n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 ['neg' 'pos']\n",
      "1800\n",
      "[0 0 0 1 1 0 1 1 0 0] 1800\n",
      "200 ['pos' 'neg']\n",
      "200\n",
      "[1 0 0 0 1 0 0 1 0 0] 200\n",
      "Convert class vector to binary 1-hot encoding matrix (for use with categorical_crossentropy)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "\n",
    "# read in the training data\n",
    "train_data = pd.read_csv('../data/sa_train.csv')\n",
    "print(len(train_data), train_data['output'].unique())\n",
    "\n",
    "train_texts = train_data['input']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(train_texts)\n",
    "\n",
    "X_train = t.texts_to_sequences(train_texts)\n",
    "print(len(X_train))\n",
    "\n",
    "# transform labels into numbers\n",
    "labels2numbers = LabelEncoder()\n",
    "\n",
    "y_train_org = labels2numbers.fit_transform(train_data['output'])\n",
    "print(y_train_org[:10], len(y_train_org))\n",
    "\n",
    "# read in test data\n",
    "test_data = pd.read_csv('../data/sa_test.csv')\n",
    "print(len(test_data), test_data['output'].unique())\n",
    "\n",
    "test_texts = test_data['input']\n",
    "\n",
    "X_test = t.texts_to_sequences(test_texts)\n",
    "print(len(X_test))\n",
    "\n",
    "y_test_org = labels2numbers.transform(test_data['output'])\n",
    "print(y_test_org[:10], len(y_test_org))\n",
    "\n",
    "# get number of classes for transformation\n",
    "num_classes = max(y_train_org) + 1\n",
    "# get vocabulary size\n",
    "vocab_size = len(t.word_index) + 1 # vocabulary size (plus reserved index 0)\n",
    "\n",
    "print('Convert class vector to binary 1-hot encoding matrix (for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train_org, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test_org, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 64)          2428672   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 48)                21696     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 96)                4704      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 194       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,455,266\n",
      "Trainable params: 2,455,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# in Keras (from keras examples):\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, CuDNNLSTM, Embedding, Flatten, initializers\n",
    "from keras.layers.core import Lambda, Dropout, Dense, Activation\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras import backend as K\n",
    "\n",
    "hidden_dims = 96\n",
    "lstm_dims = 48\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=64, \n",
    "                    input_dim=vocab_size,\n",
    "                    input_length=None))\n",
    "model.add(LSTM(lstm_dims))\n",
    "# model.add(CuDNNLSTM(lstm_dims))\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sorted([len(xx) for xx in X_train])\n",
    "f = k[int(len(k)/100*90)]\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = f\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1440 samples, validate on 360 samples\n",
      "Epoch 1/5\n",
      "1440/1440 [==============================] - 135s 94ms/step - loss: 0.6932 - acc: 0.5056 - val_loss: 0.6920 - val_acc: 0.5250\n",
      "Epoch 2/5\n",
      "1440/1440 [==============================] - 128s 89ms/step - loss: 0.6913 - acc: 0.5014 - val_loss: 0.6923 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "1440/1440 [==============================] - 124s 86ms/step - loss: 0.6839 - acc: 0.5139 - val_loss: 0.6891 - val_acc: 0.5028\n",
      "Epoch 4/5\n",
      "1440/1440 [==============================] - 222s 154ms/step - loss: 0.6682 - acc: 0.5479 - val_loss: 0.6876 - val_acc: 0.5056\n",
      "Epoch 5/5\n",
      "1440/1440 [==============================] - 168s 117ms/step - loss: 0.6424 - acc: 0.5632 - val_loss: 0.6917 - val_acc: 0.5028\n",
      "200/200 [==============================] - 4s 18ms/step\n",
      "Test loss: 0.6990297365188599\n",
      "Test accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_training(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    " \n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    " \n",
    "        return context_vector, attention_weights"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
