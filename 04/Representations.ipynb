{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Vectors\n",
    "\n",
    "## *\"I know words. I have the best words!\"*\n",
    "    - Noam Chomsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discrete Sparse Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [line.strip() for line in open('../data/moby_dick.txt', encoding='utf8')]\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/reviews.tsv', sep='\\t')\n",
    "documents = df.text.values.tolist()\n",
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences_2 = documents[:1]\n",
    "\n",
    "small_vectorizer = CountVectorizer()\n",
    "\n",
    "X1 = small_vectorizer.fit_transform(sentences_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The result is a *sparse count matrix*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexed representation\n",
    "print(X1)\n",
    "\n",
    "# dense representation\n",
    "print(X1.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can access the mapping from vector position to feature names via `get_feature_names()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inverse (the mapping from feature names to vector positions) is encoded as a list in `vocabulary_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(small_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Terminology \n",
    "\n",
    "![](../../material/pics/matrix.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's redo this for the entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0.001, max_df=0.75, stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Use vector operations to find out \n",
    "- what the 5 most frequent words are in `X`\n",
    "- in how many different documents the word `delivery` occurs\n",
    "- what percentage of the overall corpus that number corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Character $n$-grams\n",
    "\n",
    "We can also use characters to analyze text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 6), min_df=1, max_df=0.75)\n",
    "\n",
    "C = char_vectorizer.fit_transform(documents[:10])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(char_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Syntactic $n$-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [' '.join([\"{}_{}\".format(c.lemma_, c.head.lemma_) \n",
    "                      for c in nlp(sentence)])\n",
    "            for sentence in documents[:100]]\n",
    "\n",
    "syntax_vectorizer = CountVectorizer()\n",
    "X = syntax_vectorizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(syntax_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dense Distributed Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word embeddings with `Word2vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import FAST_VERSION\n",
    "\n",
    "corpus = [document.split() for document in documents]\n",
    "# initialize model\n",
    "w2v_model = Word2Vec(size=100, \n",
    "                     window=15, \n",
    "                     hs=0,\n",
    "                     sample=0.000001,\n",
    "                     negative=5, \n",
    "                     min_count=100,\n",
    "                     workers=-1, \n",
    "                     iter=100\n",
    ")\n",
    "\n",
    "w2v_model.build_vocab(corpus)\n",
    "\n",
    "w2v_model.train(corpus, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we can use the embeddings of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['delivery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# birthday - present + husband => birthday:present as husband:?\n",
    "w2v_model.wv.most_similar(positive=['birthday', 'husband'], negative=['present'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "word1 = \"birthday\"\n",
    "word2 = \"weekend\"\n",
    "\n",
    "# retrieve the actual vector\n",
    "print(w2v_model.wv[word1])\n",
    "\n",
    "# compare\n",
    "print(w2v_model.wv.similarity(word1, word2))\n",
    "\n",
    "# get the 3 most similar words\n",
    "print(w2v_model.wv.most_similar(word1, topn=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Exercise\n",
    "Use `spacy` to restrict the words in the tweets to *content words*, i.e., nouns, verbs, and adjectives. Transform the words to lower case and add the POS with an underderscore. E.g.:\n",
    "\n",
    "`love_VERB old-fashioneds_NOUN`\n",
    "\n",
    "This also allows us to distinguish between homographs, i.e., words that are written the same, but belong to different word classes, e.g., *love* in \"I **love** old-fashioneds\" vs. \"He felt so sick, it must have been **love**\".\n",
    "\n",
    "\n",
    "Make sure to exclude sentences that contain none of the above.\n",
    "\n",
    "Write the resulting corpus to a variable called `word_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the `Word2vec` model from above on the new data set and test the words out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Train 4 more `Word2vec` models and average the resulting embedding matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document embeddings with `Doc2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import FAST_VERSION\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "df2 = pd.read_csv('../data/reviews.full.tsv', sep='\\t')\n",
    "\n",
    "corpus = []\n",
    "# for docid, document in enumerate(documents):\n",
    "#     corpus.append(TaggedDocument(document.split(), tags=[\"{0:0>4}\".format(docid)]))\n",
    "for row in df2.iterrows():\n",
    "    label = row[1].score\n",
    "    text = row[1].text\n",
    "    corpus.append(TaggedDocument(text.split(), tags=[str(label)]))\n",
    "\n",
    "print('done')\n",
    "d2v_model = Doc2Vec(vector_size=100, \n",
    "                    window=15,\n",
    "                    hs=0,\n",
    "                    sample=0.000001,\n",
    "                    negative=5,\n",
    "                    min_count=100,\n",
    "                    workers=-1,\n",
    "                    epochs=500,\n",
    "                    dm=0, \n",
    "                    dbow_words=1)\n",
    "\n",
    "d2v_model.build_vocab(corpus)\n",
    "\n",
    "d2v_model.train(corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now look at the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.docvecs.doctags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_doc = '1'\n",
    "\n",
    "similar_docs = d2v_model.docvecs.most_similar(target_doc, topn=5)\n",
    "print(similar_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "What are the 10 most similar ***words*** to each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
